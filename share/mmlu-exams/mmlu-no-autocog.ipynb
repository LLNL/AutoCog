{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef981fd-a1bf-4a23-8936-127f661bc7b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prototype implemetation without AutoCog's Automaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75a92ff-4b14-4714-a6d6-e165c21d8695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, csv, copy, json\n",
    "import glob, tqdm, itertools\n",
    "import pathlib, shutil\n",
    "\n",
    "letters = ['A','B','C','D']\n",
    "\n",
    "def list_topics(datapath, mode, pattern='*'):\n",
    "    datapath = os.path.realpath(datapath)\n",
    "    basename_no_ext = lambda p: p.split('/')[-1].split('.')[0]\n",
    "    topic_and_mode_bne = lambda bnes: { 'topic' : '_'.join(bnes[:-1]), 'mode' : bnes[-1] }\n",
    "    if mode == 'aux':\n",
    "        topic_auxiliary = lambda p: topic_and_mode_bne(basename_no_ext(p).split('_') + ['aux'])\n",
    "        found = glob.glob(f\"{datapath}/auxiliary_train/{pattern}.csv\")\n",
    "        return list(map(topic_auxiliary, found))\n",
    "    else:\n",
    "        topic_and_mode = lambda p: topic_and_mode_bne(basename_no_ext(p).split('_'))\n",
    "        found = glob.glob(f\"{datapath}/{mode}/{pattern}_{mode}.csv\")\n",
    "        return list(map(topic_and_mode, found))\n",
    "\n",
    "def open_topic(topic, mode, datapath):\n",
    "    datapath = os.path.realpath(datapath)\n",
    "    data = []\n",
    "    if mode == 'aux':\n",
    "        filepath = f\"{datapath}/auxiliary_train/{topic}.csv\"\n",
    "    else:\n",
    "        filepath = f\"{datapath}/{mode}/{topic}_{mode}.csv\"\n",
    "    with open(filepath) as F:\n",
    "        for (i,row) in enumerate(csv.reader(F)):\n",
    "            data.append({ 'idx' : i, 'mode' : mode, 'topic' : topic, 'question': row[0], 'choices' : row[1:-1], 'answer' : row[-1] })\n",
    "    return data\n",
    "\n",
    "def guess_answer(llm, topic, question, choices, **kwargs):\n",
    "    prompt = \"You are answering a question for the MMLU exam.\\n\"\n",
    "    prompt += f\"Topic: {topic.replace('_', ' ')}\\n\"\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    prompt += \"Answer: \"\n",
    "    idx = llm.choose(prompt=prompt, choices=choices)\n",
    "    return letters[idx], prompt\n",
    "\n",
    "def choose_answer(llm, topic, question, choices, **kwargs):\n",
    "    prompt = \"You are answering a question for the MMLU exam.\\n\"\n",
    "    prompt += f\"Topic: {topic.replace('_', ' ')}\\n\"\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    for (l,choice) in zip(letters,choices):\n",
    "        prompt += f\"Choice {l}: {choice}\\n\"\n",
    "    prompt += \"Answer: \"\n",
    "    idx = llm.choose(prompt=prompt, choices=letters)\n",
    "    return letters[idx], prompt\n",
    "\n",
    "def repeat_answer(llm, topic, question, choices, **kwargs):\n",
    "    prompt = \"You are answering a question for the MMLU exam.\\n\"\n",
    "    prompt += f\"Topic: {topic.replace('_', ' ')}\\n\"\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    for (l,choice) in zip(letters,choices):\n",
    "        prompt += f\"Choice {l}: {choice}\\n\"\n",
    "    prompt += \"Answer: \"\n",
    "    idx = llm.choose(prompt=prompt, choices=choices)\n",
    "    return letters[idx], prompt\n",
    "\n",
    "def think_then_choose_answer(llm, topic, question, choices, max_thoughts=5, **kwargs):\n",
    "    prompt = \"You are answering a question for the MMLU exam.\\n\"\n",
    "    prompt += f\"Topic: {topic.replace('_', ' ')}\\n\"\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    for (l,choice) in zip(letters,choices):\n",
    "        prompt += f\"Choice {l}: {choice}\\n\"\n",
    "    idx = 0\n",
    "    cnt = 0\n",
    "    while idx == 0 and cnt < max_thoughts:\n",
    "        prompt += \"Draft: \"\n",
    "        text = llm.complete(prompt=prompt, stop='\\n', max_tokens=10)\n",
    "        prompt += text.strip() + \"\\n\"\n",
    "        idx = llm.choose(prompt=prompt, choices=[\"Draft: \", \"Answer: \"])\n",
    "        cnt += 1\n",
    "    prompt += \"Answer: \"\n",
    "    idx = llm.choose(prompt=prompt, choices=letters)\n",
    "    return letters[idx], prompt\n",
    "\n",
    "def think_then_repeat_answer(llm, topic, question, choices, max_thoughts=5, **kwargs):\n",
    "    prompt = \"You are answering a question for the MMLU exam.\\n\"\n",
    "    prompt += f\"Topic: {topic.replace('_', ' ')}\\n\"\n",
    "    prompt += f\"Question: {question}\\n\"\n",
    "    for (l,choice) in zip(letters,choices):\n",
    "        prompt += f\"Choice {l}: {choice}\\n\"\n",
    "    idx = 0\n",
    "    cnt = 0\n",
    "    while idx == 0 and cnt < max_thoughts:\n",
    "        prompt += \"Draft: \"\n",
    "        text = llm.complete(prompt=prompt, stop='\\n', max_tokens=10)\n",
    "        prompt += text.strip() + \"\\n\"\n",
    "        idx = llm.choose(prompt=prompt, choices=[\"Draft: \", \"Answer: \"])\n",
    "        cnt += 1\n",
    "    prompt += \"Answer: \"\n",
    "    idx = llm.choose(prompt=prompt, choices=choices)\n",
    "    return letters[idx], prompt\n",
    "\n",
    "def run_mmlu(llm, data, methods, respath):\n",
    "    pathlib.Path(respath).mkdir(parents=True, exist_ok=True)\n",
    "    #results = { f\"{sample['topic']}_{sample['idx']}\" : copy.deepcopy(sample) for sample in data }\n",
    "    prod = list(itertools.product(range(len(data)), list(methods.keys())))\n",
    "    for (s, m) in tqdm.tqdm(prod):\n",
    "        sample = data[s]\n",
    "        tag = f\"{sample['topic']}_{sample['idx']}\"\n",
    "        res = methods[m](llm, **sample)\n",
    "        res = { m : res }\n",
    "        #results[tag].update(res)\n",
    "        with open(f'{respath}/{tag}_{m}.json', 'w') as F:\n",
    "            res.update(sample)\n",
    "            json.dump(res, F, indent=4)\n",
    "    #return results\n",
    "\n",
    "methods = {\n",
    "    'guess' : guess_answer,\n",
    "    'choose' : choose_answer,\n",
    "    'repeat' : repeat_answer,\n",
    "    'think-choose' : think_then_choose_answer,\n",
    "    'think-repeat' : think_then_repeat_answer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fd8dce-b50b-4374-8dfd-5d353763d6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /workspace/models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  68.20 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 1026.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n",
      "  0%|                                                                                                                                   | 169/578500 [1:26:39<4942:15:01, 30.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m json\u001b[38;5;241m.\u001b[39mdump(data, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     15\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_mmlu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 103\u001b[0m, in \u001b[0;36mrun_mmlu\u001b[0;34m(llm, data, methods, respath)\u001b[0m\n\u001b[1;32m    101\u001b[0m sample \u001b[38;5;241m=\u001b[39m data[s]\n\u001b[1;32m    102\u001b[0m tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m res \u001b[38;5;241m=\u001b[39m { m : res }\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m#results[tag].update(res)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 88\u001b[0m, in \u001b[0;36mthink_then_repeat_answer\u001b[0;34m(llm, topic, question, choices, max_thoughts, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cnt \u001b[38;5;241m<\u001b[39m max_thoughts:\n\u001b[1;32m     87\u001b[0m     prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDraft: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 88\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     idx \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mchoose(prompt\u001b[38;5;241m=\u001b[39mprompt, choices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDraft: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/workspace/projects/autocog/autocog/lm/llama.py:30\u001b[0m, in \u001b[0;36mLlama.complete\u001b[0;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     29\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mupdate({k:v})\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m stop\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp_python-0.1.43-py3.11-linux-x86_64.egg/llama_cpp/llama.py:812\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, full_logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    776\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001b[1;32m    789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \n\u001b[1;32m    791\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp_python-0.1.43-py3.11-linux-x86_64.egg/llama_cpp/llama.py:771\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, full_logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    769\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m--> 771\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp_python-0.1.43-py3.11-linux-x86_64.egg/llama_cpp/llama.py:535\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, full_logprobs, echo, stop, repeat_penalty, top_k, stream)\u001b[0m\n\u001b[1;32m    533\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    536\u001b[0m     prompt_tokens,\n\u001b[1;32m    537\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    538\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    539\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    540\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    541\u001b[0m ):\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_eos():\n\u001b[1;32m    543\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp_python-0.1.43-py3.11-linux-x86_64.egg/llama_cpp/llama.py:414\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    416\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    417\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    418\u001b[0m         temp\u001b[38;5;241m=\u001b[39mtemp,\n\u001b[1;32m    419\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    421\u001b[0m     tokens_or_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp_python-0.1.43-py3.11-linux-x86_64.egg/llama_cpp/llama.py:232\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    230\u001b[0m n_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_tokens))\n\u001b[1;32m    231\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m--> 232\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(return_code) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_eval returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp_python-0.1.43-py3.11-linux-x86_64.egg/llama_cpp/llama_cpp.py:335\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_eval\u001b[39m(\n\u001b[1;32m    329\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    330\u001b[0m     tokens,  \u001b[38;5;66;03m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m     n_threads: c_int,\n\u001b[1;32m    334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m c_int:\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from autocog.lm import Llama\n",
    "\n",
    "llm = Llama(model_path=\"/workspace/models/7B/ggml-model-q4_0.bin\", n_ctx=2048)\n",
    "data = []\n",
    "for topic in list_topics(datapath='/workspace/datasets/MMLU', mode='dev', pattern='*'):\n",
    "    data += open_topic(datapath='/workspace/datasets/MMLU', **topic)\n",
    "for topic in list_topics(datapath='/workspace/datasets/MMLU', mode='aux', pattern='*'):\n",
    "    data += open_topic(datapath='/workspace/datasets/MMLU', **topic)\n",
    "for topic in list_topics(datapath='/workspace/datasets/MMLU', mode='val', pattern='*'):\n",
    "    data += open_topic(datapath='/workspace/datasets/MMLU', **topic)\n",
    "for topic in list_topics(datapath='/workspace/datasets/MMLU', mode='test', pattern='*'):\n",
    "    data += open_topic(datapath='/workspace/datasets/MMLU', **topic)\n",
    "\n",
    "json.dump(data, open('data.json','w'), indent=4)\n",
    "shutil.rmtree('results')\n",
    "results = run_mmlu(llm, data, methods, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c7a27-9232-48e7-9448-f47201e44f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(results, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
