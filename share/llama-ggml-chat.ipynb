{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d90f849-2ce7-470a-b2f2-fd222c17c8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Package `openai` and `tiktoken` needed for OpenAI wrapper (`pip install openai tiktoken`)\n",
      "Warning: Package `transformers` needed for (Huggingface's) transformers wrapper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /workspace/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 6828.73 MB (+ 2048.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 2048.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =  281.35 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from autocog import CogArch\n",
    "from autocog.architecture.utility import PromptTee\n",
    "from autocog.lm import Llama\n",
    "\n",
    "arch = CogArch(pipe=PromptTee(prefix='qna', tee=sys.stdout))\n",
    "llama = Llama.create(model_path=\"/workspace/llama-2-7b-chat.ggmlv3.q8_0.bin\", n_ctx=4096)\n",
    "arch.orchestrator.LMs.update({ 'text' : Llama(**llama, completion_kwargs={ 'max_tokens' : 20, 'temperature' : 0.4 }) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebea708-e244-4433-8016-939f287eb61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "program = \"\"\"\\\n",
    "entry(qna_main): This is a simple question and answer system.\n",
    "\n",
    "prompt(qna_main): Given a user question, display your work, then formulate an answer to be presented to the user.\n",
    "- target(question) source(?question)\n",
    "> question: the user asks a question\n",
    "> work[1,10](thought): use as many thoughts as needed to find the correct answer\n",
    "> answer: your standalone answer based on your thinking.\n",
    "__exit(answer):\n",
    "\"\"\"\n",
    "cog = arch.load(tag=f'qna', language='sta', program=program)\n",
    "for p in cog.prompts.values():\n",
    "    p.header = '[INST]\\n<<SYS>>{preamble}<</SYS>>\\n{automaton}\\n{prompt}\\n{basics}\\n{mechs}\\n```\\n{mechanics}```\\n{fmts}\\n{formats}\\n{postscriptum}\\n[/INST]\\nstart(record):\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26b1a42-643b-48b0-9a3e-7a3bfd3570f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " === qna[0] === \n",
      "\n",
      "[INST]\n",
      "<<SYS>>You are a helpful AI assistant.<</SYS>>\n",
      "This is a simple question and answer system.\n",
      "Given a user question, display your work, then formulate an answer to be presented to the user.\n",
      "You are using an interactive questionnaire.\n",
      "Follow this structure after the start prompt:\n",
      "```\n",
      "> question(text): the user asks a question\n",
      "> work[1,10](thought): use as many thoughts as needed to find the correct answer\n",
      "> answer(text): your standalone answer based on your thinking.\n",
      "```\n",
      "Each prompt expects one of the following formats:\n",
      "- text: ASCII text in any form\n",
      "- thought: use thought to take notes as you perform a task (a few words per lines)\n",
      "Terminate each prompt with a newline. Use as many statement with `thought` format as needed.\n",
      "[/INST]\n",
      "start(record):\n",
      "> question(text): What is the capital of France?\n",
      "> work[1](thought): ü§î Let me check... (thought)\n",
      "> work[2](thought): üá´üá∑ Paris! (thought)\n",
      "> answer(text): üëç Correct! The capital of France is indeed Paris. (text)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'üëç Correct! The capital of France is indeed Paris. (text)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await arch('qna', question='What is the capital of France?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23eaa1bc-5a31-44c8-b676-b1056f26147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " === qna[1] === \n",
      "\n",
      "[INST]\n",
      "<<SYS>>You are a helpful AI assistant.<</SYS>>\n",
      "This is a simple question and answer system.\n",
      "Given a user question, display your work, then formulate an answer to be presented to the user.\n",
      "You are using an interactive questionnaire.\n",
      "Follow this structure after the start prompt:\n",
      "```\n",
      "> question(text): the user asks a question\n",
      "> work[1,10](thought): use as many thoughts as needed to find the correct answer\n",
      "> answer(text): your standalone answer based on your thinking.\n",
      "```\n",
      "Each prompt expects one of the following formats:\n",
      "- text: ASCII text in any form\n",
      "- thought: use thought to take notes as you perform a task (a few words per lines)\n",
      "Terminate each prompt with a newline. Use as many statement with `thought` format as needed.\n",
      "[/INST]\n",
      "start(record):\n",
      "> question(text): In a room I have only 3 sisters. Anna is reading a book. Alice is playing a match of chess. What the third sister, Amanda is doing ?\n",
      "> work[1](thought): ü§î Hmm, let me think... Could she be drawing something? üé®\n",
      "> work[2](thought): ü§î Or maybe she's listening to music? üéµ\n",
      "> work[3](thought): ü§î Wait, could she be playing a game on her phone? üì±\n",
      "> work[4](thought): ü§î Hmm, I'm not sure... Could you give me more clues?\n",
      "> work[5](thought): ü§î Oh, I see! Based on the information provided, it seems that Amanda\n",
      "> answer(text): üìö is reading a book as well! üòä\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'üìö is reading a book as well! üòä'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await arch('qna', question='In a room I have only 3 sisters. Anna is reading a book. Alice is playing a match of chess. What the third sister, Amanda is doing ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f2a27-b4b0-4d56-8679-17bdedf0ec0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
